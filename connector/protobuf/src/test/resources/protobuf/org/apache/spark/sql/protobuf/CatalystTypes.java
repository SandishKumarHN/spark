// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: connector/protobuf/src/test/resources/protobuf/catalyst_types.proto

package org.apache.spark.sql.protobuf;

public final class CatalystTypes {
  private CatalystTypes() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  public interface BooleanMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.BooleanMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>bool bool_type = 1;</code>
     * @return The boolType.
     */
    boolean getBoolType();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.BooleanMsg}
   */
  public static final class BooleanMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.BooleanMsg)
      BooleanMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BooleanMsg.newBuilder() to construct.
    private BooleanMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BooleanMsg() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BooleanMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BooleanMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              boolType_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BooleanMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.Builder.class);
    }

    public static final int BOOL_TYPE_FIELD_NUMBER = 1;
    private boolean boolType_;
    /**
     * <code>bool bool_type = 1;</code>
     * @return The boolType.
     */
    @java.lang.Override
    public boolean getBoolType() {
      return boolType_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (boolType_ != false) {
        output.writeBool(1, boolType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (boolType_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, boolType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg) obj;

      if (getBoolType()
          != other.getBoolType()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + BOOL_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getBoolType());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.BooleanMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.BooleanMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BooleanMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        boolType_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg(this);
        result.boolType_ = boolType_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg.getDefaultInstance()) return this;
        if (other.getBoolType() != false) {
          setBoolType(other.getBoolType());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private boolean boolType_ ;
      /**
       * <code>bool bool_type = 1;</code>
       * @return The boolType.
       */
      @java.lang.Override
      public boolean getBoolType() {
        return boolType_;
      }
      /**
       * <code>bool bool_type = 1;</code>
       * @param value The boolType to set.
       * @return This builder for chaining.
       */
      public Builder setBoolType(boolean value) {
        
        boolType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool bool_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBoolType() {
        
        boolType_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.BooleanMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.BooleanMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<BooleanMsg>
        PARSER = new com.google.protobuf.AbstractParser<BooleanMsg>() {
      @java.lang.Override
      public BooleanMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BooleanMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BooleanMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BooleanMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.BooleanMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface IntegerMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.IntegerMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>int32 int32_type = 1;</code>
     * @return The int32Type.
     */
    int getInt32Type();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.IntegerMsg}
   */
  public static final class IntegerMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.IntegerMsg)
      IntegerMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use IntegerMsg.newBuilder() to construct.
    private IntegerMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private IntegerMsg() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new IntegerMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private IntegerMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              int32Type_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_IntegerMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.Builder.class);
    }

    public static final int INT32_TYPE_FIELD_NUMBER = 1;
    private int int32Type_;
    /**
     * <code>int32 int32_type = 1;</code>
     * @return The int32Type.
     */
    @java.lang.Override
    public int getInt32Type() {
      return int32Type_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (int32Type_ != 0) {
        output.writeInt32(1, int32Type_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (int32Type_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, int32Type_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg) obj;

      if (getInt32Type()
          != other.getInt32Type()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + INT32_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getInt32Type();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.IntegerMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.IntegerMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_IntegerMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        int32Type_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg(this);
        result.int32Type_ = int32Type_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg.getDefaultInstance()) return this;
        if (other.getInt32Type() != 0) {
          setInt32Type(other.getInt32Type());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private int int32Type_ ;
      /**
       * <code>int32 int32_type = 1;</code>
       * @return The int32Type.
       */
      @java.lang.Override
      public int getInt32Type() {
        return int32Type_;
      }
      /**
       * <code>int32 int32_type = 1;</code>
       * @param value The int32Type to set.
       * @return This builder for chaining.
       */
      public Builder setInt32Type(int value) {
        
        int32Type_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 int32_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearInt32Type() {
        
        int32Type_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.IntegerMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.IntegerMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<IntegerMsg>
        PARSER = new com.google.protobuf.AbstractParser<IntegerMsg>() {
      @java.lang.Override
      public IntegerMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new IntegerMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<IntegerMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<IntegerMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.IntegerMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface DoubleMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.DoubleMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>double double_type = 1;</code>
     * @return The doubleType.
     */
    double getDoubleType();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.DoubleMsg}
   */
  public static final class DoubleMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.DoubleMsg)
      DoubleMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DoubleMsg.newBuilder() to construct.
    private DoubleMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DoubleMsg() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new DoubleMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private DoubleMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 9: {

              doubleType_ = input.readDouble();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_DoubleMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.Builder.class);
    }

    public static final int DOUBLE_TYPE_FIELD_NUMBER = 1;
    private double doubleType_;
    /**
     * <code>double double_type = 1;</code>
     * @return The doubleType.
     */
    @java.lang.Override
    public double getDoubleType() {
      return doubleType_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (java.lang.Double.doubleToRawLongBits(doubleType_) != 0) {
        output.writeDouble(1, doubleType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (java.lang.Double.doubleToRawLongBits(doubleType_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(1, doubleType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg) obj;

      if (java.lang.Double.doubleToLongBits(getDoubleType())
          != java.lang.Double.doubleToLongBits(
              other.getDoubleType())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + DOUBLE_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getDoubleType()));
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.DoubleMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.DoubleMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_DoubleMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        doubleType_ = 0D;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg(this);
        result.doubleType_ = doubleType_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg.getDefaultInstance()) return this;
        if (other.getDoubleType() != 0D) {
          setDoubleType(other.getDoubleType());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private double doubleType_ ;
      /**
       * <code>double double_type = 1;</code>
       * @return The doubleType.
       */
      @java.lang.Override
      public double getDoubleType() {
        return doubleType_;
      }
      /**
       * <code>double double_type = 1;</code>
       * @param value The doubleType to set.
       * @return This builder for chaining.
       */
      public Builder setDoubleType(double value) {
        
        doubleType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double double_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDoubleType() {
        
        doubleType_ = 0D;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.DoubleMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.DoubleMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<DoubleMsg>
        PARSER = new com.google.protobuf.AbstractParser<DoubleMsg>() {
      @java.lang.Override
      public DoubleMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new DoubleMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<DoubleMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<DoubleMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.DoubleMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface FloatMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.FloatMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>float float_type = 1;</code>
     * @return The floatType.
     */
    float getFloatType();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.FloatMsg}
   */
  public static final class FloatMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.FloatMsg)
      FloatMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use FloatMsg.newBuilder() to construct.
    private FloatMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private FloatMsg() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new FloatMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private FloatMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 13: {

              floatType_ = input.readFloat();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_FloatMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.Builder.class);
    }

    public static final int FLOAT_TYPE_FIELD_NUMBER = 1;
    private float floatType_;
    /**
     * <code>float float_type = 1;</code>
     * @return The floatType.
     */
    @java.lang.Override
    public float getFloatType() {
      return floatType_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (java.lang.Float.floatToRawIntBits(floatType_) != 0) {
        output.writeFloat(1, floatType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (java.lang.Float.floatToRawIntBits(floatType_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(1, floatType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg) obj;

      if (java.lang.Float.floatToIntBits(getFloatType())
          != java.lang.Float.floatToIntBits(
              other.getFloatType())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + FLOAT_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + java.lang.Float.floatToIntBits(
          getFloatType());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.FloatMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.FloatMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.FloatMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_FloatMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        floatType_ = 0F;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg(this);
        result.floatType_ = floatType_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg.getDefaultInstance()) return this;
        if (other.getFloatType() != 0F) {
          setFloatType(other.getFloatType());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private float floatType_ ;
      /**
       * <code>float float_type = 1;</code>
       * @return The floatType.
       */
      @java.lang.Override
      public float getFloatType() {
        return floatType_;
      }
      /**
       * <code>float float_type = 1;</code>
       * @param value The floatType to set.
       * @return This builder for chaining.
       */
      public Builder setFloatType(float value) {
        
        floatType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>float float_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFloatType() {
        
        floatType_ = 0F;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.FloatMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.FloatMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<FloatMsg>
        PARSER = new com.google.protobuf.AbstractParser<FloatMsg>() {
      @java.lang.Override
      public FloatMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FloatMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<FloatMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<FloatMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.FloatMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BytesMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.BytesMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>bytes bytes_type = 1;</code>
     * @return The bytesType.
     */
    com.google.protobuf.ByteString getBytesType();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.BytesMsg}
   */
  public static final class BytesMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.BytesMsg)
      BytesMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BytesMsg.newBuilder() to construct.
    private BytesMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BytesMsg() {
      bytesType_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BytesMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BytesMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {

              bytesType_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BytesMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.Builder.class);
    }

    public static final int BYTES_TYPE_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString bytesType_;
    /**
     * <code>bytes bytes_type = 1;</code>
     * @return The bytesType.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getBytesType() {
      return bytesType_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!bytesType_.isEmpty()) {
        output.writeBytes(1, bytesType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!bytesType_.isEmpty()) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, bytesType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg) obj;

      if (!getBytesType()
          .equals(other.getBytesType())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + BYTES_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getBytesType().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.BytesMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.BytesMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.BytesMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BytesMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        bytesType_ = com.google.protobuf.ByteString.EMPTY;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg(this);
        result.bytesType_ = bytesType_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg.getDefaultInstance()) return this;
        if (other.getBytesType() != com.google.protobuf.ByteString.EMPTY) {
          setBytesType(other.getBytesType());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private com.google.protobuf.ByteString bytesType_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>bytes bytes_type = 1;</code>
       * @return The bytesType.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getBytesType() {
        return bytesType_;
      }
      /**
       * <code>bytes bytes_type = 1;</code>
       * @param value The bytesType to set.
       * @return This builder for chaining.
       */
      public Builder setBytesType(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        bytesType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bytes bytes_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearBytesType() {
        
        bytesType_ = getDefaultInstance().getBytesType();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.BytesMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.BytesMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<BytesMsg>
        PARSER = new com.google.protobuf.AbstractParser<BytesMsg>() {
      @java.lang.Override
      public BytesMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BytesMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BytesMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BytesMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.BytesMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface StringMsgOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.StringMsg)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string string_type = 1;</code>
     * @return The stringType.
     */
    java.lang.String getStringType();
    /**
     * <code>string string_type = 1;</code>
     * @return The bytes for stringType.
     */
    com.google.protobuf.ByteString
        getStringTypeBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.StringMsg}
   */
  public static final class StringMsg extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.StringMsg)
      StringMsgOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use StringMsg.newBuilder() to construct.
    private StringMsg(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private StringMsg() {
      stringType_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new StringMsg();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private StringMsg(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              stringType_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_StringMsg_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.Builder.class);
    }

    public static final int STRING_TYPE_FIELD_NUMBER = 1;
    private volatile java.lang.Object stringType_;
    /**
     * <code>string string_type = 1;</code>
     * @return The stringType.
     */
    @java.lang.Override
    public java.lang.String getStringType() {
      java.lang.Object ref = stringType_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        stringType_ = s;
        return s;
      }
    }
    /**
     * <code>string string_type = 1;</code>
     * @return The bytes for stringType.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStringTypeBytes() {
      java.lang.Object ref = stringType_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stringType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(stringType_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, stringType_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(stringType_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, stringType_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.StringMsg)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.StringMsg other = (org.apache.spark.sql.protobuf.CatalystTypes.StringMsg) obj;

      if (!getStringType()
          .equals(other.getStringType())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STRING_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getStringType().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.StringMsg prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.StringMsg}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.StringMsg)
        org.apache.spark.sql.protobuf.CatalystTypes.StringMsgOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_StringMsg_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.class, org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stringType_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.StringMsg getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.StringMsg build() {
        org.apache.spark.sql.protobuf.CatalystTypes.StringMsg result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.StringMsg buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.StringMsg result = new org.apache.spark.sql.protobuf.CatalystTypes.StringMsg(this);
        result.stringType_ = stringType_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.StringMsg) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.StringMsg)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.StringMsg other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.StringMsg.getDefaultInstance()) return this;
        if (!other.getStringType().isEmpty()) {
          stringType_ = other.stringType_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.StringMsg parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.StringMsg) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object stringType_ = "";
      /**
       * <code>string string_type = 1;</code>
       * @return The stringType.
       */
      public java.lang.String getStringType() {
        java.lang.Object ref = stringType_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          stringType_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string string_type = 1;</code>
       * @return The bytes for stringType.
       */
      public com.google.protobuf.ByteString
          getStringTypeBytes() {
        java.lang.Object ref = stringType_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stringType_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string string_type = 1;</code>
       * @param value The stringType to set.
       * @return This builder for chaining.
       */
      public Builder setStringType(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        stringType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string string_type = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStringType() {
        
        stringType_ = getDefaultInstance().getStringType();
        onChanged();
        return this;
      }
      /**
       * <code>string string_type = 1;</code>
       * @param value The bytes for stringType to set.
       * @return This builder for chaining.
       */
      public Builder setStringTypeBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        stringType_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.StringMsg)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.StringMsg)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.StringMsg DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.StringMsg();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.StringMsg getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<StringMsg>
        PARSER = new com.google.protobuf.AbstractParser<StringMsg>() {
      @java.lang.Override
      public StringMsg parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StringMsg(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<StringMsg> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<StringMsg> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.StringMsg getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PersonOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.Person)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>int32 age = 2;</code>
     * @return The age.
     */
    int getAge();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.Person}
   */
  public static final class Person extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.Person)
      PersonOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Person.newBuilder() to construct.
    private Person(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Person() {
      name_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new Person();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Person(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              age_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Person_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Person_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.Person.class, org.apache.spark.sql.protobuf.CatalystTypes.Person.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int AGE_FIELD_NUMBER = 2;
    private int age_;
    /**
     * <code>int32 age = 2;</code>
     * @return The age.
     */
    @java.lang.Override
    public int getAge() {
      return age_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (age_ != 0) {
        output.writeInt32(2, age_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (age_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, age_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.Person)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.Person other = (org.apache.spark.sql.protobuf.CatalystTypes.Person) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (getAge()
          != other.getAge()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + AGE_FIELD_NUMBER;
      hash = (53 * hash) + getAge();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Person parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.Person prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.Person}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.Person)
        org.apache.spark.sql.protobuf.CatalystTypes.PersonOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Person_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Person_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.Person.class, org.apache.spark.sql.protobuf.CatalystTypes.Person.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.Person.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        age_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Person_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Person getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.Person.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Person build() {
        org.apache.spark.sql.protobuf.CatalystTypes.Person result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Person buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.Person result = new org.apache.spark.sql.protobuf.CatalystTypes.Person(this);
        result.name_ = name_;
        result.age_ = age_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.Person) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.Person)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.Person other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.Person.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.getAge() != 0) {
          setAge(other.getAge());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.Person parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.Person) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int age_ ;
      /**
       * <code>int32 age = 2;</code>
       * @return The age.
       */
      @java.lang.Override
      public int getAge() {
        return age_;
      }
      /**
       * <code>int32 age = 2;</code>
       * @param value The age to set.
       * @return This builder for chaining.
       */
      public Builder setAge(int value) {
        
        age_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 age = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearAge() {
        
        age_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.Person)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.Person)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.Person DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.Person();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Person getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Person>
        PARSER = new com.google.protobuf.AbstractParser<Person>() {
      @java.lang.Override
      public Person parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Person(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<Person> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Person> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.Person getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BadOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.Bad)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>bytes col_0 = 1;</code>
     * @return The col0.
     */
    com.google.protobuf.ByteString getCol0();

    /**
     * <code>double col_1 = 2;</code>
     * @return The col1.
     */
    double getCol1();

    /**
     * <code>string col_2 = 3;</code>
     * @return The col2.
     */
    java.lang.String getCol2();
    /**
     * <code>string col_2 = 3;</code>
     * @return The bytes for col2.
     */
    com.google.protobuf.ByteString
        getCol2Bytes();

    /**
     * <code>float col_3 = 4;</code>
     * @return The col3.
     */
    float getCol3();

    /**
     * <code>int64 col_4 = 5;</code>
     * @return The col4.
     */
    long getCol4();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.Bad}
   */
  public static final class Bad extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.Bad)
      BadOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Bad.newBuilder() to construct.
    private Bad(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Bad() {
      col0_ = com.google.protobuf.ByteString.EMPTY;
      col2_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new Bad();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Bad(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {

              col0_ = input.readBytes();
              break;
            }
            case 17: {

              col1_ = input.readDouble();
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();

              col2_ = s;
              break;
            }
            case 37: {

              col3_ = input.readFloat();
              break;
            }
            case 40: {

              col4_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Bad_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Bad_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.Bad.class, org.apache.spark.sql.protobuf.CatalystTypes.Bad.Builder.class);
    }

    public static final int COL_0_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString col0_;
    /**
     * <code>bytes col_0 = 1;</code>
     * @return The col0.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getCol0() {
      return col0_;
    }

    public static final int COL_1_FIELD_NUMBER = 2;
    private double col1_;
    /**
     * <code>double col_1 = 2;</code>
     * @return The col1.
     */
    @java.lang.Override
    public double getCol1() {
      return col1_;
    }

    public static final int COL_2_FIELD_NUMBER = 3;
    private volatile java.lang.Object col2_;
    /**
     * <code>string col_2 = 3;</code>
     * @return The col2.
     */
    @java.lang.Override
    public java.lang.String getCol2() {
      java.lang.Object ref = col2_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        col2_ = s;
        return s;
      }
    }
    /**
     * <code>string col_2 = 3;</code>
     * @return The bytes for col2.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getCol2Bytes() {
      java.lang.Object ref = col2_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        col2_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int COL_3_FIELD_NUMBER = 4;
    private float col3_;
    /**
     * <code>float col_3 = 4;</code>
     * @return The col3.
     */
    @java.lang.Override
    public float getCol3() {
      return col3_;
    }

    public static final int COL_4_FIELD_NUMBER = 5;
    private long col4_;
    /**
     * <code>int64 col_4 = 5;</code>
     * @return The col4.
     */
    @java.lang.Override
    public long getCol4() {
      return col4_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!col0_.isEmpty()) {
        output.writeBytes(1, col0_);
      }
      if (java.lang.Double.doubleToRawLongBits(col1_) != 0) {
        output.writeDouble(2, col1_);
      }
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(col2_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, col2_);
      }
      if (java.lang.Float.floatToRawIntBits(col3_) != 0) {
        output.writeFloat(4, col3_);
      }
      if (col4_ != 0L) {
        output.writeInt64(5, col4_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!col0_.isEmpty()) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, col0_);
      }
      if (java.lang.Double.doubleToRawLongBits(col1_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(2, col1_);
      }
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(col2_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, col2_);
      }
      if (java.lang.Float.floatToRawIntBits(col3_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(4, col3_);
      }
      if (col4_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, col4_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.Bad)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.Bad other = (org.apache.spark.sql.protobuf.CatalystTypes.Bad) obj;

      if (!getCol0()
          .equals(other.getCol0())) return false;
      if (java.lang.Double.doubleToLongBits(getCol1())
          != java.lang.Double.doubleToLongBits(
              other.getCol1())) return false;
      if (!getCol2()
          .equals(other.getCol2())) return false;
      if (java.lang.Float.floatToIntBits(getCol3())
          != java.lang.Float.floatToIntBits(
              other.getCol3())) return false;
      if (getCol4()
          != other.getCol4()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + COL_0_FIELD_NUMBER;
      hash = (53 * hash) + getCol0().hashCode();
      hash = (37 * hash) + COL_1_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getCol1()));
      hash = (37 * hash) + COL_2_FIELD_NUMBER;
      hash = (53 * hash) + getCol2().hashCode();
      hash = (37 * hash) + COL_3_FIELD_NUMBER;
      hash = (53 * hash) + java.lang.Float.floatToIntBits(
          getCol3());
      hash = (37 * hash) + COL_4_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getCol4());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.Bad prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.Bad}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.Bad)
        org.apache.spark.sql.protobuf.CatalystTypes.BadOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Bad_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Bad_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.Bad.class, org.apache.spark.sql.protobuf.CatalystTypes.Bad.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.Bad.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        col0_ = com.google.protobuf.ByteString.EMPTY;

        col1_ = 0D;

        col2_ = "";

        col3_ = 0F;

        col4_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Bad_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Bad getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.Bad.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Bad build() {
        org.apache.spark.sql.protobuf.CatalystTypes.Bad result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Bad buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.Bad result = new org.apache.spark.sql.protobuf.CatalystTypes.Bad(this);
        result.col0_ = col0_;
        result.col1_ = col1_;
        result.col2_ = col2_;
        result.col3_ = col3_;
        result.col4_ = col4_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.Bad) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.Bad)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.Bad other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.Bad.getDefaultInstance()) return this;
        if (other.getCol0() != com.google.protobuf.ByteString.EMPTY) {
          setCol0(other.getCol0());
        }
        if (other.getCol1() != 0D) {
          setCol1(other.getCol1());
        }
        if (!other.getCol2().isEmpty()) {
          col2_ = other.col2_;
          onChanged();
        }
        if (other.getCol3() != 0F) {
          setCol3(other.getCol3());
        }
        if (other.getCol4() != 0L) {
          setCol4(other.getCol4());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.Bad parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.Bad) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private com.google.protobuf.ByteString col0_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>bytes col_0 = 1;</code>
       * @return The col0.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getCol0() {
        return col0_;
      }
      /**
       * <code>bytes col_0 = 1;</code>
       * @param value The col0 to set.
       * @return This builder for chaining.
       */
      public Builder setCol0(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        col0_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bytes col_0 = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol0() {
        
        col0_ = getDefaultInstance().getCol0();
        onChanged();
        return this;
      }

      private double col1_ ;
      /**
       * <code>double col_1 = 2;</code>
       * @return The col1.
       */
      @java.lang.Override
      public double getCol1() {
        return col1_;
      }
      /**
       * <code>double col_1 = 2;</code>
       * @param value The col1 to set.
       * @return This builder for chaining.
       */
      public Builder setCol1(double value) {
        
        col1_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double col_1 = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol1() {
        
        col1_ = 0D;
        onChanged();
        return this;
      }

      private java.lang.Object col2_ = "";
      /**
       * <code>string col_2 = 3;</code>
       * @return The col2.
       */
      public java.lang.String getCol2() {
        java.lang.Object ref = col2_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          col2_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string col_2 = 3;</code>
       * @return The bytes for col2.
       */
      public com.google.protobuf.ByteString
          getCol2Bytes() {
        java.lang.Object ref = col2_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          col2_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string col_2 = 3;</code>
       * @param value The col2 to set.
       * @return This builder for chaining.
       */
      public Builder setCol2(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        col2_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string col_2 = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol2() {
        
        col2_ = getDefaultInstance().getCol2();
        onChanged();
        return this;
      }
      /**
       * <code>string col_2 = 3;</code>
       * @param value The bytes for col2 to set.
       * @return This builder for chaining.
       */
      public Builder setCol2Bytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        col2_ = value;
        onChanged();
        return this;
      }

      private float col3_ ;
      /**
       * <code>float col_3 = 4;</code>
       * @return The col3.
       */
      @java.lang.Override
      public float getCol3() {
        return col3_;
      }
      /**
       * <code>float col_3 = 4;</code>
       * @param value The col3 to set.
       * @return This builder for chaining.
       */
      public Builder setCol3(float value) {
        
        col3_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>float col_3 = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol3() {
        
        col3_ = 0F;
        onChanged();
        return this;
      }

      private long col4_ ;
      /**
       * <code>int64 col_4 = 5;</code>
       * @return The col4.
       */
      @java.lang.Override
      public long getCol4() {
        return col4_;
      }
      /**
       * <code>int64 col_4 = 5;</code>
       * @param value The col4 to set.
       * @return This builder for chaining.
       */
      public Builder setCol4(long value) {
        
        col4_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int64 col_4 = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol4() {
        
        col4_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.Bad)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.Bad)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.Bad DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.Bad();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Bad getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Bad>
        PARSER = new com.google.protobuf.AbstractParser<Bad>() {
      @java.lang.Override
      public Bad parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Bad(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<Bad> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Bad> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.Bad getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ActualOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.Actual)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string col_0 = 1;</code>
     * @return The col0.
     */
    java.lang.String getCol0();
    /**
     * <code>string col_0 = 1;</code>
     * @return The bytes for col0.
     */
    com.google.protobuf.ByteString
        getCol0Bytes();

    /**
     * <code>int32 col_1 = 2;</code>
     * @return The col1.
     */
    int getCol1();

    /**
     * <code>float col_2 = 3;</code>
     * @return The col2.
     */
    float getCol2();

    /**
     * <code>bool col_3 = 4;</code>
     * @return The col3.
     */
    boolean getCol3();

    /**
     * <code>double col_4 = 5;</code>
     * @return The col4.
     */
    double getCol4();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.Actual}
   */
  public static final class Actual extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.Actual)
      ActualOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Actual.newBuilder() to construct.
    private Actual(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Actual() {
      col0_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new Actual();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Actual(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              col0_ = s;
              break;
            }
            case 16: {

              col1_ = input.readInt32();
              break;
            }
            case 29: {

              col2_ = input.readFloat();
              break;
            }
            case 32: {

              col3_ = input.readBool();
              break;
            }
            case 41: {

              col4_ = input.readDouble();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Actual_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Actual_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.Actual.class, org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder.class);
    }

    public static final int COL_0_FIELD_NUMBER = 1;
    private volatile java.lang.Object col0_;
    /**
     * <code>string col_0 = 1;</code>
     * @return The col0.
     */
    @java.lang.Override
    public java.lang.String getCol0() {
      java.lang.Object ref = col0_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        col0_ = s;
        return s;
      }
    }
    /**
     * <code>string col_0 = 1;</code>
     * @return The bytes for col0.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getCol0Bytes() {
      java.lang.Object ref = col0_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        col0_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int COL_1_FIELD_NUMBER = 2;
    private int col1_;
    /**
     * <code>int32 col_1 = 2;</code>
     * @return The col1.
     */
    @java.lang.Override
    public int getCol1() {
      return col1_;
    }

    public static final int COL_2_FIELD_NUMBER = 3;
    private float col2_;
    /**
     * <code>float col_2 = 3;</code>
     * @return The col2.
     */
    @java.lang.Override
    public float getCol2() {
      return col2_;
    }

    public static final int COL_3_FIELD_NUMBER = 4;
    private boolean col3_;
    /**
     * <code>bool col_3 = 4;</code>
     * @return The col3.
     */
    @java.lang.Override
    public boolean getCol3() {
      return col3_;
    }

    public static final int COL_4_FIELD_NUMBER = 5;
    private double col4_;
    /**
     * <code>double col_4 = 5;</code>
     * @return The col4.
     */
    @java.lang.Override
    public double getCol4() {
      return col4_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(col0_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, col0_);
      }
      if (col1_ != 0) {
        output.writeInt32(2, col1_);
      }
      if (java.lang.Float.floatToRawIntBits(col2_) != 0) {
        output.writeFloat(3, col2_);
      }
      if (col3_ != false) {
        output.writeBool(4, col3_);
      }
      if (java.lang.Double.doubleToRawLongBits(col4_) != 0) {
        output.writeDouble(5, col4_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(col0_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, col0_);
      }
      if (col1_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, col1_);
      }
      if (java.lang.Float.floatToRawIntBits(col2_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(3, col2_);
      }
      if (col3_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, col3_);
      }
      if (java.lang.Double.doubleToRawLongBits(col4_) != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(5, col4_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.Actual)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.Actual other = (org.apache.spark.sql.protobuf.CatalystTypes.Actual) obj;

      if (!getCol0()
          .equals(other.getCol0())) return false;
      if (getCol1()
          != other.getCol1()) return false;
      if (java.lang.Float.floatToIntBits(getCol2())
          != java.lang.Float.floatToIntBits(
              other.getCol2())) return false;
      if (getCol3()
          != other.getCol3()) return false;
      if (java.lang.Double.doubleToLongBits(getCol4())
          != java.lang.Double.doubleToLongBits(
              other.getCol4())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + COL_0_FIELD_NUMBER;
      hash = (53 * hash) + getCol0().hashCode();
      hash = (37 * hash) + COL_1_FIELD_NUMBER;
      hash = (53 * hash) + getCol1();
      hash = (37 * hash) + COL_2_FIELD_NUMBER;
      hash = (53 * hash) + java.lang.Float.floatToIntBits(
          getCol2());
      hash = (37 * hash) + COL_3_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getCol3());
      hash = (37 * hash) + COL_4_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getCol4()));
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.Actual prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.Actual}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.Actual)
        org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Actual_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Actual_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.Actual.class, org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.Actual.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        col0_ = "";

        col1_ = 0;

        col2_ = 0F;

        col3_ = false;

        col4_ = 0D;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_Actual_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Actual getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.Actual.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Actual build() {
        org.apache.spark.sql.protobuf.CatalystTypes.Actual result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.Actual buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.Actual result = new org.apache.spark.sql.protobuf.CatalystTypes.Actual(this);
        result.col0_ = col0_;
        result.col1_ = col1_;
        result.col2_ = col2_;
        result.col3_ = col3_;
        result.col4_ = col4_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.Actual) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.Actual)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.Actual other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.Actual.getDefaultInstance()) return this;
        if (!other.getCol0().isEmpty()) {
          col0_ = other.col0_;
          onChanged();
        }
        if (other.getCol1() != 0) {
          setCol1(other.getCol1());
        }
        if (other.getCol2() != 0F) {
          setCol2(other.getCol2());
        }
        if (other.getCol3() != false) {
          setCol3(other.getCol3());
        }
        if (other.getCol4() != 0D) {
          setCol4(other.getCol4());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.Actual parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.Actual) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object col0_ = "";
      /**
       * <code>string col_0 = 1;</code>
       * @return The col0.
       */
      public java.lang.String getCol0() {
        java.lang.Object ref = col0_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          col0_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string col_0 = 1;</code>
       * @return The bytes for col0.
       */
      public com.google.protobuf.ByteString
          getCol0Bytes() {
        java.lang.Object ref = col0_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          col0_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string col_0 = 1;</code>
       * @param value The col0 to set.
       * @return This builder for chaining.
       */
      public Builder setCol0(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        col0_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string col_0 = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol0() {
        
        col0_ = getDefaultInstance().getCol0();
        onChanged();
        return this;
      }
      /**
       * <code>string col_0 = 1;</code>
       * @param value The bytes for col0 to set.
       * @return This builder for chaining.
       */
      public Builder setCol0Bytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        col0_ = value;
        onChanged();
        return this;
      }

      private int col1_ ;
      /**
       * <code>int32 col_1 = 2;</code>
       * @return The col1.
       */
      @java.lang.Override
      public int getCol1() {
        return col1_;
      }
      /**
       * <code>int32 col_1 = 2;</code>
       * @param value The col1 to set.
       * @return This builder for chaining.
       */
      public Builder setCol1(int value) {
        
        col1_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 col_1 = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol1() {
        
        col1_ = 0;
        onChanged();
        return this;
      }

      private float col2_ ;
      /**
       * <code>float col_2 = 3;</code>
       * @return The col2.
       */
      @java.lang.Override
      public float getCol2() {
        return col2_;
      }
      /**
       * <code>float col_2 = 3;</code>
       * @param value The col2 to set.
       * @return This builder for chaining.
       */
      public Builder setCol2(float value) {
        
        col2_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>float col_2 = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol2() {
        
        col2_ = 0F;
        onChanged();
        return this;
      }

      private boolean col3_ ;
      /**
       * <code>bool col_3 = 4;</code>
       * @return The col3.
       */
      @java.lang.Override
      public boolean getCol3() {
        return col3_;
      }
      /**
       * <code>bool col_3 = 4;</code>
       * @param value The col3 to set.
       * @return This builder for chaining.
       */
      public Builder setCol3(boolean value) {
        
        col3_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>bool col_3 = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol3() {
        
        col3_ = false;
        onChanged();
        return this;
      }

      private double col4_ ;
      /**
       * <code>double col_4 = 5;</code>
       * @return The col4.
       */
      @java.lang.Override
      public double getCol4() {
        return col4_;
      }
      /**
       * <code>double col_4 = 5;</code>
       * @param value The col4 to set.
       * @return This builder for chaining.
       */
      public Builder setCol4(double value) {
        
        col4_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>double col_4 = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearCol4() {
        
        col4_ = 0D;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.Actual)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.Actual)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.Actual DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.Actual();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.Actual getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Actual>
        PARSER = new com.google.protobuf.AbstractParser<Actual>() {
      @java.lang.Override
      public Actual parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Actual(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<Actual> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Actual> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.Actual getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface oldConsumerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.oldConsumer)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    java.lang.String getKey();
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    com.google.protobuf.ByteString
        getKeyBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.oldConsumer}
   */
  public static final class oldConsumer extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.oldConsumer)
      oldConsumerOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use oldConsumer.newBuilder() to construct.
    private oldConsumer(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private oldConsumer() {
      key_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new oldConsumer();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private oldConsumer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              key_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldConsumer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.class, org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.Builder.class);
    }

    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    @java.lang.Override
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        key_ = s;
        return s;
      }
    }
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer other = (org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer) obj;

      if (!getKey()
          .equals(other.getKey())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + KEY_FIELD_NUMBER;
      hash = (53 * hash) + getKey().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.oldConsumer}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.oldConsumer)
        org.apache.spark.sql.protobuf.CatalystTypes.oldConsumerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldConsumer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.class, org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer build() {
        org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer result = new org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer(this);
        result.key_ = key_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer.getDefaultInstance()) return this;
        if (!other.getKey().isEmpty()) {
          key_ = other.key_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object key_ = "";
      /**
       * <code>string key = 1;</code>
       * @return The key.
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @return The bytes for key.
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @param value The key to set.
       * @return This builder for chaining.
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKey() {
        
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @param value The bytes for key to set.
       * @return This builder for chaining.
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        key_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.oldConsumer)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.oldConsumer)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<oldConsumer>
        PARSER = new com.google.protobuf.AbstractParser<oldConsumer>() {
      @java.lang.Override
      public oldConsumer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new oldConsumer(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<oldConsumer> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<oldConsumer> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.oldConsumer getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface newProducerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.newProducer)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    java.lang.String getKey();
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    com.google.protobuf.ByteString
        getKeyBytes();

    /**
     * <code>int32 value = 2;</code>
     * @return The value.
     */
    int getValue();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.newProducer}
   */
  public static final class newProducer extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.newProducer)
      newProducerOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use newProducer.newBuilder() to construct.
    private newProducer(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private newProducer() {
      key_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new newProducer();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private newProducer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              key_ = s;
              break;
            }
            case 16: {

              value_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newProducer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.newProducer.class, org.apache.spark.sql.protobuf.CatalystTypes.newProducer.Builder.class);
    }

    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    @java.lang.Override
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        key_ = s;
        return s;
      }
    }
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 2;
    private int value_;
    /**
     * <code>int32 value = 2;</code>
     * @return The value.
     */
    @java.lang.Override
    public int getValue() {
      return value_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      if (value_ != 0) {
        output.writeInt32(2, value_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      if (value_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, value_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.newProducer)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.newProducer other = (org.apache.spark.sql.protobuf.CatalystTypes.newProducer) obj;

      if (!getKey()
          .equals(other.getKey())) return false;
      if (getValue()
          != other.getValue()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + KEY_FIELD_NUMBER;
      hash = (53 * hash) + getKey().hashCode();
      hash = (37 * hash) + VALUE_FIELD_NUMBER;
      hash = (53 * hash) + getValue();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.newProducer prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.newProducer}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.newProducer)
        org.apache.spark.sql.protobuf.CatalystTypes.newProducerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newProducer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.newProducer.class, org.apache.spark.sql.protobuf.CatalystTypes.newProducer.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.newProducer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";

        value_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newProducer getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.newProducer.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newProducer build() {
        org.apache.spark.sql.protobuf.CatalystTypes.newProducer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newProducer buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.newProducer result = new org.apache.spark.sql.protobuf.CatalystTypes.newProducer(this);
        result.key_ = key_;
        result.value_ = value_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.newProducer) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.newProducer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.newProducer other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.newProducer.getDefaultInstance()) return this;
        if (!other.getKey().isEmpty()) {
          key_ = other.key_;
          onChanged();
        }
        if (other.getValue() != 0) {
          setValue(other.getValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.newProducer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.newProducer) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object key_ = "";
      /**
       * <code>string key = 1;</code>
       * @return The key.
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @return The bytes for key.
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @param value The key to set.
       * @return This builder for chaining.
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKey() {
        
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @param value The bytes for key to set.
       * @return This builder for chaining.
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        key_ = value;
        onChanged();
        return this;
      }

      private int value_ ;
      /**
       * <code>int32 value = 2;</code>
       * @return The value.
       */
      @java.lang.Override
      public int getValue() {
        return value_;
      }
      /**
       * <code>int32 value = 2;</code>
       * @param value The value to set.
       * @return This builder for chaining.
       */
      public Builder setValue(int value) {
        
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 value = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue() {
        
        value_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.newProducer)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.newProducer)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.newProducer DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.newProducer();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.newProducer getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<newProducer>
        PARSER = new com.google.protobuf.AbstractParser<newProducer>() {
      @java.lang.Override
      public newProducer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new newProducer(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<newProducer> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<newProducer> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.newProducer getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface newConsumerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.newConsumer)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    java.lang.String getKey();
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    com.google.protobuf.ByteString
        getKeyBytes();

    /**
     * <code>int32 value = 2;</code>
     * @return The value.
     */
    int getValue();

    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     * @return Whether the actual field is set.
     */
    boolean hasActual();
    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     * @return The actual.
     */
    org.apache.spark.sql.protobuf.CatalystTypes.Actual getActual();
    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     */
    org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder getActualOrBuilder();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.newConsumer}
   */
  public static final class newConsumer extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.newConsumer)
      newConsumerOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use newConsumer.newBuilder() to construct.
    private newConsumer(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private newConsumer() {
      key_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new newConsumer();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private newConsumer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              key_ = s;
              break;
            }
            case 16: {

              value_ = input.readInt32();
              break;
            }
            case 26: {
              org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder subBuilder = null;
              if (actual_ != null) {
                subBuilder = actual_.toBuilder();
              }
              actual_ = input.readMessage(org.apache.spark.sql.protobuf.CatalystTypes.Actual.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(actual_);
                actual_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newConsumer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.class, org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.Builder.class);
    }

    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    @java.lang.Override
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        key_ = s;
        return s;
      }
    }
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 2;
    private int value_;
    /**
     * <code>int32 value = 2;</code>
     * @return The value.
     */
    @java.lang.Override
    public int getValue() {
      return value_;
    }

    public static final int ACTUAL_FIELD_NUMBER = 3;
    private org.apache.spark.sql.protobuf.CatalystTypes.Actual actual_;
    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     * @return Whether the actual field is set.
     */
    @java.lang.Override
    public boolean hasActual() {
      return actual_ != null;
    }
    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     * @return The actual.
     */
    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.Actual getActual() {
      return actual_ == null ? org.apache.spark.sql.protobuf.CatalystTypes.Actual.getDefaultInstance() : actual_;
    }
    /**
     * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
     */
    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder getActualOrBuilder() {
      return getActual();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      if (value_ != 0) {
        output.writeInt32(2, value_);
      }
      if (actual_ != null) {
        output.writeMessage(3, getActual());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      if (value_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, value_);
      }
      if (actual_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getActual());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.newConsumer)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.newConsumer other = (org.apache.spark.sql.protobuf.CatalystTypes.newConsumer) obj;

      if (!getKey()
          .equals(other.getKey())) return false;
      if (getValue()
          != other.getValue()) return false;
      if (hasActual() != other.hasActual()) return false;
      if (hasActual()) {
        if (!getActual()
            .equals(other.getActual())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + KEY_FIELD_NUMBER;
      hash = (53 * hash) + getKey().hashCode();
      hash = (37 * hash) + VALUE_FIELD_NUMBER;
      hash = (53 * hash) + getValue();
      if (hasActual()) {
        hash = (37 * hash) + ACTUAL_FIELD_NUMBER;
        hash = (53 * hash) + getActual().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.newConsumer prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.newConsumer}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.newConsumer)
        org.apache.spark.sql.protobuf.CatalystTypes.newConsumerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newConsumer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.class, org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";

        value_ = 0;

        if (actualBuilder_ == null) {
          actual_ = null;
        } else {
          actual_ = null;
          actualBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newConsumer getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newConsumer build() {
        org.apache.spark.sql.protobuf.CatalystTypes.newConsumer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.newConsumer buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.newConsumer result = new org.apache.spark.sql.protobuf.CatalystTypes.newConsumer(this);
        result.key_ = key_;
        result.value_ = value_;
        if (actualBuilder_ == null) {
          result.actual_ = actual_;
        } else {
          result.actual_ = actualBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.newConsumer) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.newConsumer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.newConsumer other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.newConsumer.getDefaultInstance()) return this;
        if (!other.getKey().isEmpty()) {
          key_ = other.key_;
          onChanged();
        }
        if (other.getValue() != 0) {
          setValue(other.getValue());
        }
        if (other.hasActual()) {
          mergeActual(other.getActual());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.newConsumer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.newConsumer) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object key_ = "";
      /**
       * <code>string key = 1;</code>
       * @return The key.
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @return The bytes for key.
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @param value The key to set.
       * @return This builder for chaining.
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKey() {
        
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @param value The bytes for key to set.
       * @return This builder for chaining.
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        key_ = value;
        onChanged();
        return this;
      }

      private int value_ ;
      /**
       * <code>int32 value = 2;</code>
       * @return The value.
       */
      @java.lang.Override
      public int getValue() {
        return value_;
      }
      /**
       * <code>int32 value = 2;</code>
       * @param value The value to set.
       * @return This builder for chaining.
       */
      public Builder setValue(int value) {
        
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>int32 value = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue() {
        
        value_ = 0;
        onChanged();
        return this;
      }

      private org.apache.spark.sql.protobuf.CatalystTypes.Actual actual_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.protobuf.CatalystTypes.Actual, org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder, org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder> actualBuilder_;
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       * @return Whether the actual field is set.
       */
      public boolean hasActual() {
        return actualBuilder_ != null || actual_ != null;
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       * @return The actual.
       */
      public org.apache.spark.sql.protobuf.CatalystTypes.Actual getActual() {
        if (actualBuilder_ == null) {
          return actual_ == null ? org.apache.spark.sql.protobuf.CatalystTypes.Actual.getDefaultInstance() : actual_;
        } else {
          return actualBuilder_.getMessage();
        }
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public Builder setActual(org.apache.spark.sql.protobuf.CatalystTypes.Actual value) {
        if (actualBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          actual_ = value;
          onChanged();
        } else {
          actualBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public Builder setActual(
          org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder builderForValue) {
        if (actualBuilder_ == null) {
          actual_ = builderForValue.build();
          onChanged();
        } else {
          actualBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public Builder mergeActual(org.apache.spark.sql.protobuf.CatalystTypes.Actual value) {
        if (actualBuilder_ == null) {
          if (actual_ != null) {
            actual_ =
              org.apache.spark.sql.protobuf.CatalystTypes.Actual.newBuilder(actual_).mergeFrom(value).buildPartial();
          } else {
            actual_ = value;
          }
          onChanged();
        } else {
          actualBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public Builder clearActual() {
        if (actualBuilder_ == null) {
          actual_ = null;
          onChanged();
        } else {
          actual_ = null;
          actualBuilder_ = null;
        }

        return this;
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder getActualBuilder() {
        
        onChanged();
        return getActualFieldBuilder().getBuilder();
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      public org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder getActualOrBuilder() {
        if (actualBuilder_ != null) {
          return actualBuilder_.getMessageOrBuilder();
        } else {
          return actual_ == null ?
              org.apache.spark.sql.protobuf.CatalystTypes.Actual.getDefaultInstance() : actual_;
        }
      }
      /**
       * <code>.org.apache.spark.sql.protobuf.Actual actual = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.sql.protobuf.CatalystTypes.Actual, org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder, org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder> 
          getActualFieldBuilder() {
        if (actualBuilder_ == null) {
          actualBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.sql.protobuf.CatalystTypes.Actual, org.apache.spark.sql.protobuf.CatalystTypes.Actual.Builder, org.apache.spark.sql.protobuf.CatalystTypes.ActualOrBuilder>(
                  getActual(),
                  getParentForChildren(),
                  isClean());
          actual_ = null;
        }
        return actualBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.newConsumer)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.newConsumer)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.newConsumer DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.newConsumer();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.newConsumer getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<newConsumer>
        PARSER = new com.google.protobuf.AbstractParser<newConsumer>() {
      @java.lang.Override
      public newConsumer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new newConsumer(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<newConsumer> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<newConsumer> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.newConsumer getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface oldProducerOrBuilder extends
      // @@protoc_insertion_point(interface_extends:org.apache.spark.sql.protobuf.oldProducer)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    java.lang.String getKey();
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    com.google.protobuf.ByteString
        getKeyBytes();
  }
  /**
   * Protobuf type {@code org.apache.spark.sql.protobuf.oldProducer}
   */
  public static final class oldProducer extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:org.apache.spark.sql.protobuf.oldProducer)
      oldProducerOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use oldProducer.newBuilder() to construct.
    private oldProducer(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private oldProducer() {
      key_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new oldProducer();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private oldProducer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              key_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldProducer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.class, org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.Builder.class);
    }

    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>string key = 1;</code>
     * @return The key.
     */
    @java.lang.Override
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        key_ = s;
        return s;
      }
    }
    /**
     * <code>string key = 1;</code>
     * @return The bytes for key.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(key_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.sql.protobuf.CatalystTypes.oldProducer)) {
        return super.equals(obj);
      }
      org.apache.spark.sql.protobuf.CatalystTypes.oldProducer other = (org.apache.spark.sql.protobuf.CatalystTypes.oldProducer) obj;

      if (!getKey()
          .equals(other.getKey())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + KEY_FIELD_NUMBER;
      hash = (53 * hash) + getKey().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.sql.protobuf.CatalystTypes.oldProducer prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code org.apache.spark.sql.protobuf.oldProducer}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:org.apache.spark.sql.protobuf.oldProducer)
        org.apache.spark.sql.protobuf.CatalystTypes.oldProducerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldProducer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.class, org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.Builder.class);
      }

      // Construct using org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldProducer getDefaultInstanceForType() {
        return org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldProducer build() {
        org.apache.spark.sql.protobuf.CatalystTypes.oldProducer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.spark.sql.protobuf.CatalystTypes.oldProducer buildPartial() {
        org.apache.spark.sql.protobuf.CatalystTypes.oldProducer result = new org.apache.spark.sql.protobuf.CatalystTypes.oldProducer(this);
        result.key_ = key_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.sql.protobuf.CatalystTypes.oldProducer) {
          return mergeFrom((org.apache.spark.sql.protobuf.CatalystTypes.oldProducer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.sql.protobuf.CatalystTypes.oldProducer other) {
        if (other == org.apache.spark.sql.protobuf.CatalystTypes.oldProducer.getDefaultInstance()) return this;
        if (!other.getKey().isEmpty()) {
          key_ = other.key_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.spark.sql.protobuf.CatalystTypes.oldProducer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.spark.sql.protobuf.CatalystTypes.oldProducer) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object key_ = "";
      /**
       * <code>string key = 1;</code>
       * @return The key.
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          key_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @return The bytes for key.
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string key = 1;</code>
       * @param value The key to set.
       * @return This builder for chaining.
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKey() {
        
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>string key = 1;</code>
       * @param value The bytes for key to set.
       * @return This builder for chaining.
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        key_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:org.apache.spark.sql.protobuf.oldProducer)
    }

    // @@protoc_insertion_point(class_scope:org.apache.spark.sql.protobuf.oldProducer)
    private static final org.apache.spark.sql.protobuf.CatalystTypes.oldProducer DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.sql.protobuf.CatalystTypes.oldProducer();
    }

    public static org.apache.spark.sql.protobuf.CatalystTypes.oldProducer getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<oldProducer>
        PARSER = new com.google.protobuf.AbstractParser<oldProducer>() {
      @java.lang.Override
      public oldProducer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new oldProducer(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<oldProducer> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<oldProducer> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.spark.sql.protobuf.CatalystTypes.oldProducer getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_BooleanMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_IntegerMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_DoubleMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_FloatMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_BytesMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_StringMsg_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_Person_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_Person_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_Bad_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_Bad_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_Actual_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_Actual_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_oldConsumer_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_newProducer_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_newConsumer_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_org_apache_spark_sql_protobuf_oldProducer_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\nCconnector/protobuf/src/test/resources/" +
      "protobuf/catalyst_types.proto\022\035org.apach" +
      "e.spark.sql.protobuf\"\037\n\nBooleanMsg\022\021\n\tbo" +
      "ol_type\030\001 \001(\010\" \n\nIntegerMsg\022\022\n\nint32_typ" +
      "e\030\001 \001(\005\" \n\tDoubleMsg\022\023\n\013double_type\030\001 \001(" +
      "\001\"\036\n\010FloatMsg\022\022\n\nfloat_type\030\001 \001(\002\"\036\n\010Byt" +
      "esMsg\022\022\n\nbytes_type\030\001 \001(\014\" \n\tStringMsg\022\023" +
      "\n\013string_type\030\001 \001(\t\"#\n\006Person\022\014\n\004name\030\001 " +
      "\001(\t\022\013\n\003age\030\002 \001(\005\"P\n\003Bad\022\r\n\005col_0\030\001 \001(\014\022\r" +
      "\n\005col_1\030\002 \001(\001\022\r\n\005col_2\030\003 \001(\t\022\r\n\005col_3\030\004 " +
      "\001(\002\022\r\n\005col_4\030\005 \001(\003\"S\n\006Actual\022\r\n\005col_0\030\001 " +
      "\001(\t\022\r\n\005col_1\030\002 \001(\005\022\r\n\005col_2\030\003 \001(\002\022\r\n\005col" +
      "_3\030\004 \001(\010\022\r\n\005col_4\030\005 \001(\001\"\032\n\013oldConsumer\022\013" +
      "\n\003key\030\001 \001(\t\")\n\013newProducer\022\013\n\003key\030\001 \001(\t\022" +
      "\r\n\005value\030\002 \001(\005\"`\n\013newConsumer\022\013\n\003key\030\001 \001" +
      "(\t\022\r\n\005value\030\002 \001(\005\0225\n\006actual\030\003 \001(\0132%.org." +
      "apache.spark.sql.protobuf.Actual\"\032\n\013oldP" +
      "roducer\022\013\n\003key\030\001 \001(\tB\017B\rCatalystTypesb\006p" +
      "roto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_org_apache_spark_sql_protobuf_BooleanMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_BooleanMsg_descriptor,
        new java.lang.String[] { "BoolType", });
    internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_org_apache_spark_sql_protobuf_IntegerMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_IntegerMsg_descriptor,
        new java.lang.String[] { "Int32Type", });
    internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_org_apache_spark_sql_protobuf_DoubleMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_DoubleMsg_descriptor,
        new java.lang.String[] { "DoubleType", });
    internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_org_apache_spark_sql_protobuf_FloatMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_FloatMsg_descriptor,
        new java.lang.String[] { "FloatType", });
    internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_org_apache_spark_sql_protobuf_BytesMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_BytesMsg_descriptor,
        new java.lang.String[] { "BytesType", });
    internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_org_apache_spark_sql_protobuf_StringMsg_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_StringMsg_descriptor,
        new java.lang.String[] { "StringType", });
    internal_static_org_apache_spark_sql_protobuf_Person_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_org_apache_spark_sql_protobuf_Person_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_Person_descriptor,
        new java.lang.String[] { "Name", "Age", });
    internal_static_org_apache_spark_sql_protobuf_Bad_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_org_apache_spark_sql_protobuf_Bad_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_Bad_descriptor,
        new java.lang.String[] { "Col0", "Col1", "Col2", "Col3", "Col4", });
    internal_static_org_apache_spark_sql_protobuf_Actual_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_org_apache_spark_sql_protobuf_Actual_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_Actual_descriptor,
        new java.lang.String[] { "Col0", "Col1", "Col2", "Col3", "Col4", });
    internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_org_apache_spark_sql_protobuf_oldConsumer_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_oldConsumer_descriptor,
        new java.lang.String[] { "Key", });
    internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_org_apache_spark_sql_protobuf_newProducer_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_newProducer_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_org_apache_spark_sql_protobuf_newConsumer_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_newConsumer_descriptor,
        new java.lang.String[] { "Key", "Value", "Actual", });
    internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_org_apache_spark_sql_protobuf_oldProducer_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_org_apache_spark_sql_protobuf_oldProducer_descriptor,
        new java.lang.String[] { "Key", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
